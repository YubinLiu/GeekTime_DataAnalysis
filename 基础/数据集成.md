### 数据集成
将多个数据源合并存放在一个数据存储中（如数据仓库）。

#### 数据集成的两种架构
* ELT：趋势  
  * Extract：数据抽取，将数据从已有的数据源中提取出来。
  * Transform：数据转换，对原始数据进行处理。
    * 表输入 1 和表输入 2 连接形成一张表。
    ![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/two_tables.png)
    * 三张表的连接。
    ![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/three_tables.png)
  * Load：数据加载

* ETL：主流
  * 提取、加载、变换（顺序不同）。
  * 抽取后将结果写入目的地，利用数据库的聚合分析能力或者外部计算框架，如 Spark 来完成转换的步骤。

* 比较
  * ELT，提取完成后，数据加载立即开始。省时，允许 BI 人员无限制地访问整个原始数据。
  * ELT，数据变换根据后续使用情况，在 SQL 中进行，可从数据源中提取数据，经过少量预处理后进行加载。

#### Kettle 工具的使用
***
**Transformation：转换。**
  * 相当于一个容器，对数据操作进行了定义。
  * 数据操作就是数据从输入到输出的过程。
  * 任务分解为不同的作业，作业分解成多个转换。

**Job：作业。**
  * 相比于转换更大的容器，负责将转换组织起来完成某项作业。
***
**如何创建 Transformation**
* 包括输入、中间转换及输出
* 两个概念
  * Step（步骤）：转换的最小单元，如下图包括表输入、值映射、去除重复记录、表输出。
  ![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/step_img.png)
  * Hop（跳跃）：转换中连接 Step，代表数据的流向。
  ![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/hop_img.jpg)

**如何创建 Job**
* 将创建好的转换和作业串联起来。
* 两个概念
  * Job Entry（工作实体）：Job 内部的执行单元，每一个都用来执行具体的任务，如调用转换、发送邮件等。
  * Hop：连接 Job Entry 的线，指定是否有条件的执行。
***
#### 阿里开源软件 DataX
实现跨平台、跨数据库、不同系统之间的数据同步及交互。以自己作为标准，连接了不同的数据源，以完成它们之间的转换。
![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/DataX.jpg)
![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/DataX框架.jpg)

#### Apache 开源软件 Sqoop
将数据从关系型数据库导入 HDFS 中，或者将数据从 HDFS 到处到关系型数据库中。
