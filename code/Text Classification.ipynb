{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn 中的 3 个朴素贝叶斯分类算法\n",
    "### 高斯朴素贝叶斯(GaussianNB)\n",
    "特征变量是连续变量，符合高斯分布，如：人的身高，物体的长度。（正态分布）\n",
    "\n",
    "### 多项式朴素贝叶斯(MultinomialNB)\n",
    "特征变量是离散变量，符合多项分布，在文档分类中体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。\n",
    "\n",
    "### 伯努利朴素贝叶斯(BernoulliNB)\n",
    "特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。\n",
    "\n",
    "**注：**\n",
    "* 这三个类适用的分类场景各不相同，一般来说，如果样本特征的分布大部分是连续值，使用 GaussianNB 会比较好。\n",
    "* 如果样本特征的分大部分是多元离散值，使用 MultinomialNB 比较合适。\n",
    "* 如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用 BernoulliNB。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 值\n",
    "用于评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。\n",
    "\n",
    "### 词频 TF(Term Frequency)\n",
    "计算了一个单词在文档中的出现次数。认为：**一个单词的重要性和它在文档中出现的次数成正比。**\n",
    "\n",
    "### 逆向文档频率 IDF(Inverse Document Frequency)\n",
    "指一个单词在文档中的区分度。认为：**一个单词出现在的文档数越少，越能够通过这个单词把该文档和其它文档区分开。IDF 越大代表该单词的区分度越大。**\n",
    "\n",
    "### TF-IDF 实际上是 TF 和 IDF 的乘积\n",
    "倾向于找到某个单词，在一个文档中出现次数多，同时又很少出现在其它文档中。这种单词适合做分类。\n",
    "\n",
    "### 计算公式\n",
    "* 词频 TF = 单词出现的次数 / 该文档的总单词数\n",
    "* 逆向文档频率 IDF = log (文档总数 / 该单词出现的文档数 + 1) \n",
    "\n",
    "*注：+1 是为了防止分母为 0，即某些单词不出现在文档中。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer 类的创建\n",
    "sklearn 中的 TfidfVectorizer 类，计算单词的 TF-IDF 值，对数底为 e。\n",
    "\n",
    "```\n",
    "TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)\n",
    "```\n",
    "* stop_words：自定义停用词列表（停用词即为分类中没有用的词，TF 高，IDF 低），List 型。\n",
    "* token_pattern：过滤规则，正则表达式。\n",
    "\n",
    "### fit_transform(X)\n",
    "* 拟合模型，返回文本矩阵。\n",
    "\n",
    "* 属性：\n",
    " * vocabulary_：词汇表，字典型。\n",
    " * idf_：返回 idf 值。\n",
    " * stop_words_：返回停用词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "documents = [\n",
    "    \"this is the bayes document\",\n",
    "    \"this is the second document\",\n",
    "    \"and the third one\",\n",
    "    \"is this the document\"\n",
    "]\n",
    "\n",
    "tfidf_matrix = tfidf_vec.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不重复的词:  ['and', 'bayes', 'document', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(\"不重复的词: \", tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个单词对应的 ID:  {'this': 8, 'is': 3, 'the': 6, 'bayes': 1, 'document': 2, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "# 输出每个单词对应的 ID\n",
    "print(\"每个单词对应的 ID: \", tfidf_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个单词的 TF-IDF 值:  [[0.         0.63314609 0.40412895 0.40412895 0.         0.\n",
      "  0.33040189 0.         0.40412895]\n",
      " [0.         0.         0.40412895 0.40412895 0.         0.63314609\n",
      "  0.33040189 0.         0.40412895]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.         0.52210862 0.52210862 0.         0.\n",
      "  0.42685801 0.         0.52210862]]\n"
     ]
    }
   ],
   "source": [
    "# 输出每个单词在每个文档中的 TF-IDF 值，向量顺序按词语的 ID 顺序\n",
    "print(\"每个单词的 TF-IDF 值: \", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档分类\n",
    "* **基于分词的数据准备**：包括分词、去掉停用词、单词权重计算。\n",
    "* **应用朴素贝叶斯分类进行分类**：通过训练集得到朴素贝叶斯分类器，将分类器应用于测试集，与实际结果对比，得到测试集的分类准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对文档进行分词\n",
    "* 英文文档：NLTK 包。\n",
    "* 中文文档：jieba 包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('carry', 'VBP'),\n",
       " ('fear', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('accepting', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('type', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('fear', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('carry', 'VBP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('first', 'JJ'),\n",
       " ('step', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('pushing', 'VBG'),\n",
       " ('past', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"We all carry fear, and accepting the type of fear you carry is the first step in pushing past it.\"\n",
    "word_list = nltk.word_tokenize(text) # 分词\n",
    "nltk.pos_tag(word_list) # 标注单词的词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "text_cn = \"大漠孤烟直，长河落日圆。\"\n",
    "word_list = jieba.cut(text_cn) # 中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载停用词表\n",
    "* 网上找到中文停用词表 stop_words.txt，利用 Python 读取后保存在 stop_words 数组中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path_zh = \"data/stopwords_zh.txt\"\n",
    "stop_words_zh = [line.strip() for line in open(file_path_zh, 'r', encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算单词的权重\n",
    "* 创建 TfidfVectorizer 类，使用 fit_transform() 拟合，得到 TF-IDF 特征空间 features，选出来的分词就是特征。\n",
    "* max_df 用来描述单词在文档中的最高出现率。\n",
    " * max_df = 0.5，代表一个单词在 50% 的文档中都出现过，那么它只携带非常少的信息，不做分词统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'al', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "file_path_en = \"data/stopwords_en.txt\"\n",
    "stop_words_en = [line.strip() for line in open(file_path_en, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "train_contents = [\"Huawei unveils Harmony operating system, won't ditch Android for smartphones.\", \n",
    "                  \"There are the 7 most common types of fear\"]\n",
    "tf = TfidfVectorizer(stop_words=stop_words_en, max_df=0.5)\n",
    "features = tf.fit_transform(train_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成朴素贝叶斯分类器\n",
    "* 将特征训练集的特征空间 train_features 和 训练集对应的分类 train_labels 传递给贝叶斯分类器 clf。\n",
    "* 以多项式贝叶斯分类器为例：\n",
    " * alpha：平滑参数，如处理某个未在训练样本中出现的单词。\n",
    "   * alpha=1，Laplace 平滑，即采用 +1 的方式，统计没有出现过的单词的概率，当训练样本很大时，+1 得到的概率变化可以忽略不计，同时避免零概率问题。\n",
    "   * 0<alpha<1，Lidstone 平滑，alpha 越小，迭代次数越多，精度越高，可设置为 0.001。\n",
    " * 例： 多项式贝叶斯分类器\n",
    " \n",
    " ```Python\n",
    " from sklearn.naive_bayes import MultinomialNB\n",
    " \n",
    " clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用生成的分类器做预测\n",
    "* 得到测试集的特征矩阵\n",
    "\n",
    "```Python\n",
    "test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)\n",
    "test_features=test_tf.fit_transform(test_contents)\n",
    "```\n",
    "\n",
    "* 用训练好的分类器对新数据进行预测：predict()\n",
    "  * 求解所有后验概率，并找到最大的那个\n",
    "\n",
    "```Python\n",
    "predicted_labels=clf.predict(test_features)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算准确率\n",
    "* 实际结果与预测结果对比，给出模型的准确率\n",
    "\n",
    "```Python\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.accuracy_score(test_labels, predicted_labels))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战：中文文档分类\n",
    "* 数据见：data/text_classification。\n",
    "* 文档有 4 类：女性、体育、文学、校园。\n",
    "* train 文件夹：训练集；test 文件夹：测试集；stop 文件夹：停用词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "def cut_words(file_path):\n",
    "    \"\"\"\n",
    "    对文本分词\n",
    "    :param file_path: txt文本路径\n",
    "    :return: 用空格分词的字符串\n",
    "    \"\"\"\n",
    "    text_with_spaces = \"\"\n",
    "    text = open(file_path, \"r\", encoding=\"gb18030\").read()\n",
    "    text_cut = jieba.cut(text)\n",
    "    for word in text_cut:\n",
    "        text_with_spaces += word + \" \"\n",
    "    return text_with_spaces\n",
    "\n",
    "def load_file(file_dir, label):\n",
    "    \"\"\"\n",
    "    加载路径下的所有文件\n",
    "    :param file_dir: 保存 txt 文件的目录\n",
    "    :param label: 文档标签\n",
    "    :return 分词后的文档列表和标签\n",
    "    \"\"\"\n",
    "    file_list = os.listdir(file_dir)\n",
    "    words_list = []\n",
    "    labels_list = []\n",
    "    for file in file_list:\n",
    "        file_path = file_dir + \"/\" + file\n",
    "        words_list.append(cut_words(file_path))\n",
    "        labels_list.append(label)\n",
    "    return words_list, labels_list\n",
    "\n",
    "# 训练数据\n",
    "train_words_women, train_labels_women = load_file(\"data/text_classification/train/女性\", \"女性\")\n",
    "train_words_sports, train_labels_sports = load_file(\"data/text_classification/train/体育\", \"体育\")\n",
    "train_words_literature, train_labels_literature = load_file(\"data/text_classification/train/文学\", \"文学\")\n",
    "train_words_campus, train_labels_campus = load_file(\"data/text_classification/train/校园\", \"校园\")\n",
    "\n",
    "train_words_list = train_words_women + train_words_sports + train_words_literature + train_words_campus\n",
    "train_labels = train_labels_women + train_labels_sports + train_labels_literature + train_labels_campus\n",
    "\n",
    "# 测试数据\n",
    "test_words_women, test_labels_women = load_file(\"data/text_classification/test/女性\", \"女性\")\n",
    "test_words_sports, test_labels_sports = load_file(\"data/text_classification/test/体育\", \"体育\")\n",
    "test_words_literature, test_labels_literature = load_file(\"data/text_classification/test/文学\", \"文学\")\n",
    "test_words_campus, test_labels_campus = load_file(\"data/text_classification/test/校园\", \"校园\")\n",
    "\n",
    "test_words_list = test_words_women + test_words_sports + test_words_literature + test_words_campus\n",
    "test_labels = test_labels_women + test_labels_sports + test_labels_literature + test_labels_campus\n",
    "\n",
    "# 停用词\n",
    "stop_words = open(\"data/text_classification/stop/stopword.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "stop_words = stop_words.encode(\"utf-8\").decode(\"utf-8-sig\")\n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "\n",
    "# 计算单词权重\n",
    "tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\n",
    "\n",
    "train_features = tf.fit_transform(train_words_list)\n",
    "test_features = tf.transform(test_words_list)\n",
    "\n",
    "# 多项式贝叶斯分类器\n",
    "clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n",
    "predicted_labels = clf.predict(test_features)\n",
    "\n",
    "# 计算准确率\n",
    "metrics.accuracy_score(test_labels, predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
