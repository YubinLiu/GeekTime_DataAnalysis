### 决策树：ID3 和 C4.5
#### 构造和剪枝
* **构造**：选择什么属性作为节点的过程。
  1. 根节点：树的最顶端，最开始的节点。
  2. 内部节点：树中间的那些节点。
  3. 叶节点：树最底部的那些节点，即决策结果。


* **剪枝**：防止过拟合，泛化能力差。
  1. 预剪枝：Pre-Pruning，决策树构造时剪枝。
    * 构造过程中评估节点是否在验证集中带来准确性的提升，若不能，则对该节点进行划分无意义，将其作为叶节点。
  2. 后剪枝：Post-Pruning，生成决策树后进行剪枝。
    * 从叶节点开始，逐层向上对每个节点进行评估。
    * 若剪掉这个节点的子树，在验证集中带来准确度的提升，或与保留时差别不大，就该把这个节点的子树进行剪枝。
    * 用这个节点子树的叶子节点替代该节点，类标记为这个节点子树中最频繁的类。


#### 决策树判断去不去打篮球
* **纯度**
  * 决策树的构造 -> 寻找纯净划分的过程 -> 让目标变量分歧最小
  * 例：
    * 集合 1：6 次都去打球
    * 集合 2：4 次去打球，2 次不去
    * 集合 3：3 次去，3 次不去
    * 纯度：集合 1 > 集合 2 > 集合 3，1 的分歧最小，3 的分歧最大


* **信息熵**
  * ![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/信息熵.png)
  * 信息的不确定度
  * p(i|t)：节点 t 为分类 i 的概率
  * 不确定性越大，包含的信息量越大，信息熵越高
  * 例：
    * 集合 1：5 次打球，1 次不打
    ![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/熵计算结果1.png)
    * 集合 2：3 次打球，3 次不打
    ![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/熵计算结果2.png)
  * 信息熵越大，纯度越低；所有样本均匀混合时，信息熵最大，纯度最低。

***
* **ID3算法**
  * 计算信息增益：划分可以带来纯度的提高，熵的下降。
  * 父节点的信息熵减去所有子节点的信息熵。
  * 归一化，按照每个子节点在父节点中出现的概率，计算它们的信息熵。
    * D：父节点
    * Di：子节点
    * Gain(D, a)：a 作为节点 D 的属性选择
    ![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/信息增益.png)
    * 例
      * D：天气 = 晴。D1：刮风 = 是。D2：刮风 = 否。a：属性节点（天气 = 晴）。
      ![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/信息增益举例图.jpg)
      * D 的信息增益
      ![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/D-信息增益.png)
***
* **ID3算法示例**
![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/天气-篮球-ID3.png)

**步骤一：计算各属性作为根节点的信息增益**
* 根节点的信息熵
  * Ent(D) = -(3/7 * log2(3/7) + 4/7 * log2(4/7)) = 0.985
* 根据天气属性划分：晴天 D1、阴天 D2、小雨 D3
  * Ent(D1) = -(1/3 * log2(1/3) + 2/3 * log(2/3)) = 0.918
  * Ent(D2) = 1.0
  * Ent(D3) = 1.0
* 子节点归一化信息熵
  * 3/7 * 0.918 + 2/7 * 1 + 2/7 * 1 = 0.965
* Gain(D, 天气) = 0.985 - 0.965 = 0.020
* 同理计算
  * Gain(D, 湿度) = 0.128
  * Gain(D, 湿度) = 0.020
  * Gain(D, 刮风) = 0.020
* 综上：温度作为根节点
![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/温度作为根节点.jpg)

**步骤二：对 D1={1-, 2-, 3+, 4+} 往下划分，计算信息增益，选取节点**
* 属性：天气、湿度、刮风
  * Gain(D, 天气) = 1
  * Gain(D, 湿度) = 1
  * Gain(D, 刮风) = 0.311
* 选择 天气/湿度 作为该节点

**步骤三：类推，得出最终结果**
![](https://github.com/YubinLiu/GeekTime_DataAnalysis/blob/master/img/ID3-最终决策树.jpg)

**缺陷**
* 有些属性对分类没有太大作用，但仍可能被选为最佳属性
***

* **C4.5算法**
  * 采用信息增益率：信息增益 / 信息熵。
  * 采用悲观剪枝：后剪枝的一种，提升决策树的泛化能力。
    * 通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。
  * 离散化处理连续属性。
    * 选择具有最高信息增益的划分所对应的的阈值。
  * 处理缺失值。


* **ID3 和 C4.5 比较**
1. ID3
  * 优点：简单
  * 缺点：对噪声敏感
2. C4.5
  * 优点：解决噪声敏感问题；可对构造树进行剪枝、处理连续值、缺失值。
  * 缺点：对收集多次扫描，效率低。
